{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41858f39",
   "metadata": {},
   "source": [
    "\n",
    "# üöó Cars Dataset ‚Äî Linear Regression (Step-by-Step, Beginner Friendly)\n",
    "*Generated on 2025-09-12 03:29:55*\n",
    "\n",
    "This notebook teaches **Linear Regression** with a Cars dataset in **small, easy steps**.  \n",
    "Each step ends with a short **üìù TODO** so you can practice.\n",
    "\n",
    "What you'll do:\n",
    "1) Download + load the dataset (via **gdown**)  \n",
    "2) Inspect + clean data (simple)  \n",
    "3) Univariate plots (histograms)  \n",
    "4) Single-variable Linear Regression (fit + plot)  \n",
    "5) Multi-variable Linear Regression (fit + metrics)  \n",
    "6) Diagnostic plots (residuals)  \n",
    "7) Polynomial features + **GridSearchCV** to find the **best degree**  \n",
    "8) Brief coefficient interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdffcadd",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Install & import (safe to re-run)\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkg):\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
    "\n",
    "pip_install(\"gdown>=5.1\")\n",
    "pip_install(\"pandas>=1.5\")\n",
    "pip_install(\"numpy>=1.23\")\n",
    "pip_install(\"matplotlib>=3.7\")\n",
    "pip_install(\"scikit-learn>=1.3\")\n",
    "\n",
    "import gdown, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ee8a7",
   "metadata": {},
   "source": [
    "## 1) Download & Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Download CSV via gdown\n",
    "GDOWN_ID = \"1bwRmKkPwmLKiqOgQ_LnKH0Vsc3mJKmVR\"  # provided ID\n",
    "OUTPUT_CSV = \"cars.csv\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_CSV):\n",
    "    url = f\"https://drive.google.com/uc?id={GDOWN_ID}\"\n",
    "    gdown.download(url, OUTPUT_CSV, quiet=False)\n",
    "else:\n",
    "    print(\"Found existing file:\", OUTPUT_CSV)\n",
    "\n",
    "df = pd.read_csv(OUTPUT_CSV)\n",
    "print(\"‚úÖ Loaded:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48462d17",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- Skim the first few rows above. What looks like a good **target** (y) for prediction (e.g., `mpg`, `price`)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d4e37",
   "metadata": {},
   "source": [
    "## 2) Quick Inspect & Simple Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
    "\n",
    "# Simple cleaning: drop duplicates\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "print(\"\\nDropped duplicates:\", before - df.shape[0])\n",
    "\n",
    "# (Optional) strip/underscore column names\n",
    "df.columns = [c.strip().replace(\" \", \"_\") for c in df.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ecb52",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- If you see obvious bad rows (e.g., impossible negative values), write one **extra line** to filter them out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93a674",
   "metadata": {},
   "source": [
    "## 3) Choose Target and Features (Keep it Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80003962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üëâ Set your target (change this if needed)\n",
    "TARGET = \"mpg\"  # <-- change if your dataset uses a different target\n",
    "\n",
    "# If the target isn't present, try to auto-pick a numeric column\n",
    "if TARGET not in df.columns:\n",
    "    numeric_cols_all = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    if numeric_cols_all:\n",
    "        TARGET = numeric_cols_all[0]\n",
    "        print(\"Auto-selected TARGET =\", TARGET)\n",
    "\n",
    "# We'll work **only with numeric predictors** for simplicity.\n",
    "numeric_cols = [c for c in df.select_dtypes(include=np.number).columns if c != TARGET]\n",
    "\n",
    "# Quick sanity\n",
    "print(\"Target:\", TARGET)\n",
    "print(\"Numeric features (first 6 shown):\", numeric_cols[:6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfa346",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- If you prefer different features, create a **manual list**, e.g.  \n",
    "  `numeric_cols = ['horsepower','weight','displacement','acceleration']` (only if those exist).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c216e",
   "metadata": {},
   "source": [
    "## 4) Univariate Plots (Matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0248ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot target histogram\n",
    "if TARGET in df.columns and pd.api.types.is_numeric_dtype(df[TARGET]):\n",
    "    plt.figure()\n",
    "    df[TARGET].plot(kind='hist', bins=30, title=f\"Histogram of {TARGET}\")\n",
    "    plt.xlabel(TARGET); plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot up to 3 numeric features' histograms\n",
    "for col in numeric_cols[:3]:\n",
    "    plt.figure()\n",
    "    df[col].plot(kind='hist', bins=30, title=f\"Histogram of {col}\")\n",
    "    plt.xlabel(col); plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32de8ec9",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- Looking at the histograms, note any skewed variables or outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615337b",
   "metadata": {},
   "source": [
    "## 5) Single-Variable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a2d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick one feature (auto-pick the first numeric feature)\n",
    "if len(numeric_cols) == 0:\n",
    "    raise ValueError(\"No numeric predictors found. Please adjust `numeric_cols`.\")\n",
    "\n",
    "FEATURE_X = numeric_cols[0]  # change to try others\n",
    "\n",
    "# Drop rows with missing target/feature\n",
    "data_1v = df[[FEATURE_X, TARGET]].dropna().copy()\n",
    "\n",
    "X = data_1v[[FEATURE_X]].values\n",
    "y = data_1v[TARGET].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Feature:\", FEATURE_X)\n",
    "print(\"Coefficient (slope):\", lr.coef_[0])\n",
    "print(\"Intercept:\", lr.intercept_)\n",
    "print(\"R^2 (test):\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Plot scatter + regression line (on test set)\n",
    "plt.figure()\n",
    "plt.scatter(X_test, y_test, label=\"Actual\")\n",
    "# create a line\n",
    "x_line = np.linspace(X.min(), X.max(), 200).reshape(-1,1)\n",
    "y_line = lr.predict(x_line)\n",
    "plt.plot(x_line, y_line, label=\"Fit\")\n",
    "plt.xlabel(FEATURE_X); plt.ylabel(TARGET); plt.title(\"Single-Variable Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c813aaf",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- Change `FEATURE_X` to a different column and re-run.\n",
    "- Does the line slope match your intuition about the relationship?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8880fb",
   "metadata": {},
   "source": [
    "## 6) Multi-Variable Linear Regression (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep only numeric predictors + target; drop NA\n",
    "data_mv = df[numeric_cols + [TARGET]].dropna().copy()\n",
    "\n",
    "X = data_mv[numeric_cols].values\n",
    "y = data_mv[TARGET].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lin.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE:  {mae:.3f}\")\n",
    "print(f\"MSE:  {mse:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R^2:  {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2800867",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- Remove one weak feature from `numeric_cols` and see how metrics change.\n",
    "- Add a different feature and compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fe2a1",
   "metadata": {},
   "source": [
    "## 7) Diagnostic Plots (Residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reuse y_test and y_pred from multi-variable model\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual\"); plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Predicted vs Actual (Test)\")\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs Predicted\n",
    "plt.figure()\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(0, linestyle='--')\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predicted\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure()\n",
    "pd.Series(residuals).plot(kind='hist', bins=30, title=\"Histogram of Residuals\")\n",
    "plt.xlabel(\"Residual\"); plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d515f1e",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- Do residuals look roughly centered around 0 and evenly spread?\n",
    "- If not, which assumption might be violated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fbe0a3",
   "metadata": {},
   "source": [
    "## 8) Polynomial Features + Grid Search for Best Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9418dd4",
   "metadata": {},
   "source": [
    "\n",
    "Even though it's called \"polynomial regression\", it's still **linear regression** applied to **polynomially-expanded features**.\n",
    "We'll try degrees **1 to 5** on a **single predictor** and pick the degree with the best cross-validated **R¬≤**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose a single predictor again (same FEATURE_X as before by default)\n",
    "FEATURE_X = FEATURE_X  # keep same, or set to something else like 'horsepower'\n",
    "\n",
    "poly_data = df[[FEATURE_X, TARGET]].dropna().copy()\n",
    "X = poly_data[[FEATURE_X]].values\n",
    "y = poly_data[TARGET].values\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"poly__degree\": [1, 2, 3, 4, 5],\n",
    "    \"lr__fit_intercept\": [True, False]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring=\"r2\", n_jobs=-1)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Plot best curve\n",
    "x_line = np.linspace(X.min(), X.max(), 200).reshape(-1,1)\n",
    "y_line = best_model.predict(x_line)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=12)\n",
    "plt.plot(x_line, y_line)\n",
    "plt.xlabel(FEATURE_X); plt.ylabel(TARGET)\n",
    "plt.title(\"Best Polynomial Fit (via Grid Search)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b23f2",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- Change the degree range (e.g., 1‚Äì8) and re-run. Does performance keep improving?\n",
    "- Try a different `FEATURE_X`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5cd04",
   "metadata": {},
   "source": [
    "## 9) Interpreting Coefficients (Multi-Variable Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show top coefficients by magnitude (multi-variable linear model)\n",
    "coef = lin.coef_\n",
    "coef_df = pd.DataFrame({\"feature\": numeric_cols, \"coefficient\": coef})\n",
    "coef_df[\"abs_coef\"] = coef_df[\"coefficient\"].abs()\n",
    "coef_df.sort_values(\"abs_coef\", ascending=False, inplace=True)\n",
    "coef_df.drop(columns=[\"abs_coef\"], inplace=True)\n",
    "coef_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069016a",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- Which features have the largest (absolute) coefficients?\n",
    "- Do the signs (+/-) match your intuition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4273bb",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Linear Regression ‚Äî Quick Assumptions Checklist\n",
    "- **Linearity**: Relationship between predictors and target is roughly linear.\n",
    "- **Independence**: Errors are independent.\n",
    "- **Homoscedasticity**: Residuals have constant variance.\n",
    "- **Normality (for inference)**: Residuals are roughly normal.\n",
    "- **No perfect multicollinearity**: Avoid duplicate/linearly dependent features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d56ff",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Assumption Checks ‚Äî Code You Can Run\n",
    "We'll check the classic linear regression assumptions using simple, readable code:\n",
    "- **Linearity & Homoscedasticity:** residuals vs predicted plot\n",
    "- **Normality of residuals:** Q‚ÄìQ plot and Shapiro‚ÄìWilk test\n",
    "- **Independence of errors:** Durbin‚ÄìWatson statistic\n",
    "- **Multicollinearity:** Variance Inflation Factor (VIF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d819a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Install test libs (statsmodels) if needed\n",
    "import sys, subprocess\n",
    "def pip_install(pkg):\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
    "\n",
    "try:\n",
    "    import statsmodels\n",
    "except:\n",
    "    pip_install(\"statsmodels>=0.14\")\n",
    "    import statsmodels\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# NOTE: This cell assumes you've already run the multi-variable model section\n",
    "# so that y_test, y_pred, X_train, X_test, numeric_cols, TARGET are defined.\n",
    "# If not, re-run sections 6 and 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11.1 Linearity & Homoscedasticity (visual)\n",
    "# Residuals vs Predicted should look like a random cloud around 0 (no pattern / fanning)\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_pred, residuals, s=14)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predicted\")\n",
    "plt.show()\n",
    "\n",
    "print(\"üßê Look for: no obvious curve/pattern; spread roughly constant across x-axis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11.2 Normality of residuals: Q‚ÄìQ plot + Shapiro‚ÄìWilk test\n",
    "plt.figure()\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q‚ÄìQ Plot of Residuals\")\n",
    "plt.show()\n",
    "\n",
    "sh_stat, sh_p = stats.shapiro(residuals)\n",
    "print(f\"Shapiro‚ÄìWilk: statistic={sh_stat:.3f}, p-value={sh_p:.3g}\")\n",
    "print(\"Rule of thumb: p-value > 0.05 suggests residuals are close to normal (for inference).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff584073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11.3 Independence of errors: Durbin‚ÄìWatson (‚âà2 is good; <1 or >3 indicates strong autocorrelation)\n",
    "dw = durbin_watson(residuals)\n",
    "print(f\"Durbin‚ÄìWatson statistic = {dw:.3f}\")\n",
    "print(\"Guideline: ~2 means uncorrelated; much below 2 ‚áí positive autocorrelation; much above 2 ‚áí negative autocorrelation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e46ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11.4 Homoscedasticity formal test: Breusch‚ÄìPagan\n",
    "# Use original predictors (X_test). Add constant for the test design matrix.\n",
    "\n",
    "X_bp = sm.add_constant(pd.DataFrame(X_test, columns=numeric_cols))\n",
    "bp_stat, bp_p, _, _ = het_breuschpagan(residuals, X_bp)\n",
    "print(f\"Breusch‚ÄìPagan: stat={bp_stat:.3f}, p-value={bp_p:.3g}\")\n",
    "print(\"Rule of thumb: p-value > 0.05 ‚áí no strong evidence of heteroscedasticity.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11.5 Multicollinearity: VIF (on training predictors)\n",
    "X_vif = pd.DataFrame(X_train, columns=numeric_cols).dropna()\n",
    "X_vif_const = sm.add_constant(X_vif, has_constant='add')\n",
    "\n",
    "vif_vals = []\n",
    "for i, col in enumerate(X_vif_const.columns):\n",
    "    if col == 'const':\n",
    "        continue\n",
    "    vif_vals.append({\"feature\": col, \"VIF\": variance_inflation_factor(X_vif_const.values, i+1)})\n",
    "\n",
    "vif_df = pd.DataFrame(vif_vals).sort_values(\"VIF\", ascending=False)\n",
    "vif_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901800b4",
   "metadata": {},
   "source": [
    "\n",
    "### üìù TODO\n",
    "- If **Breusch‚ÄìPagan p < 0.05**, try transforming a skewed feature (e.g., `np.log1p(x)`) and re-fit.  \n",
    "- If **Shapiro p < 0.05**, consider outliers or feature transforms.  \n",
    "- If **VIF > 10** for a feature, try removing it or combining highly correlated features.  \n",
    "- If **Durbin‚ÄìWatson** is far from 2 (time-indexed data), try adding lag features or using time-series models.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
